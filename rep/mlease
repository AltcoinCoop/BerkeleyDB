Design Document for Master Leases

A. Why do we need master leases at all?

Consider the following scenario:

A, B, C, D, and E form a replication group.
At time 0, A is the master.
At time 1, the network partitions leaving A on one side and B through E
on the other.

In our current system, B through E will now hold an election; let's say
they elect B.

We now have two masters, A and B, each responding to read requests.  (A
cannot respond to write requests, because it will not hear from a
majority of its replicas to commit, but the problem here has to do with
master read consistency.)

If the application needs consistent reads, it's reasonable to direct
those reads to a master, because the replicas could be delayed, but even
then, in this scenario, a reader at A can be given out of date data.

Google implemented master leases to avoid this problem.  The lease
mechanism prevents B through E from holding a successful election until
A's master lease expires, even though A is no longer in communication
with the other replicas.

B. Implementing master leases in Berkeley DB.

Note: When we talk about elections, we talk about having at least
nsites/2 + 1 sites participate.  When we talk about master leases,
we need the same idea: a majority of the sites agree on who the
master is.  However, with respect to master leases, the master itself
always thinks that it is the master, so it need only have nsites/2
replicas grant the lease to it in order to retain it.  Keep that in
mind as you read the design.

1. API changes:

Before calling dbenv->rep_set_master_lease (described below), the
application must call dbenv->rep_set_nsites(DB_ENV *dbenv, int nsites)
to specify the number of sites in the replication group.   In addition,
all calls to dbenv->rep_elect must use nsites=0 in that call, so that
the elections and master leases agree on the size of the replication
group. The nsites value set in dbenv->rep_set_nsites will be used to
control elections and to determine the number of sites needed for the
master to retain its lease.

	dbenv->rep_set_master_lease(DB_ENV *dbenv,
	    u_int32_t timeout, u_int32_t clock_scale_factor)


If this is never called, we do nothing differently than we do today.

If this is called then we do three things differently:

	* Turning master leases on -- requiring that in order to respond
	  to a read request, a master hold a master lease by means of
	  nsites/2 replicas having granted the master that lease within
	  the last timeout interval.

	* Establishing a lease timeout interval of "timeout"

	* Setting the group's clock_scale_factor (see below).

The timeout is the length of time that a replica grants a lease to the
master.  A replica promises not to participate in an election while
it has granted a lease to the master.

The master promises not to respond to read requests unless at least
nsites/2  of the replicas have granted the master a lease within the
timeout interval.

2. Optional API change

This design is written as if whenever you turn master leases on, ALL
reads to the master require that the lease be held.  We could make this
enforcement optional.  If we did that, then we would need one of two new
flags to db->get and dbc->{p}get: either one that says "Ignore leases"
or one that says "Respect leases"

3. All sites in a replication group must agree on the
master_lease_timeout and the clock_scale_factor (which is the maximum
difference in clock rates in the group -- more detail below).

4. Enforcing and adhering to the lease mechanism.

The master maintains a global view of lease status while each replica
maintains only a local view.   Let's start with the replica.

Whenever the replica sends an acknowledgement, it sets it lease timeout
value.  A replica will never start or participate in an election until
that lease expires.  For example, let's say that the lease timeout is
5000 ms.  If the replica sends an acknowledgement to the master at time
T, then the replica will not participate in an election until T+5000.

The master side is a bit more complicated.  For each replica, the master
maintains state indicating its perception of the replica's timeout.
When the master needs to respond to read requests, it examines the lease
state for all the replicas and responds only if it perceives that at
least nsites/2  of the replicas are still granting the master a lease.

(In the discussion below, assume a global clock; we'll relax that in the
next section.)

Again, consider a lease timeout value of 5000.

Let's say that the master has 4 replicas and the master sent messages
to replicas at the times below and that each of the replicas acked
those messages:

	R1	10,000
	R2	11,000
	R3	12,000
	R4	13,000

At time 14,000, the master happily responds to reads: it has 4 valid
lease grants from the replicas (5 sites still think the master is the
master).

At time 15,001, the master sill responds to reads, because it has
valid lease grants from R2-4 (4 of 5 sites still think that the
master is the master).

At time 16,999, only the master has lease grants only from
R3 and R4.  However, that means that the master and R3 and R4
are all in agreement, so the master can still respond (3 of 5
sites still agree on the master).

However, at time 17001, only R4 is still granting its lease.  That means
that R1-3 could elect a new master.  Therefore the current master cannot
respond to a read request without refreshing someone's lease.

We'll discuss eager versus lazy lease refresh below.

5. Lease Maintenance

In Google's case, application send and receive functions perform the
lease maintenance activities of:

	Replicas sending acks to the master
	Master maintenance of replica lease values
	Lease refresh

For us to implement this, we will have to handle this in
__rep_send_message and __rep_process_message.

New state:

In replica:	timestamp containing the last time the replica granted
		a lease to the master.
In master:	an entry for each replica that contains:

		Msg-ID	Time_of_Msg-ID		Lease-timeout

		Msg-ID uniquely identifies the last log message that
		the master sent to this replica.  The Time_of_Msg-ID
		is the time at which this message was sent.

		Lease-timeout is the master-local time that the
		grant from this replica expires.

In __rep_process_message:
	Replica:	in response to a log record or a request for
			an ACK:
			1. update timestamp to current time + timeout.
			2. send an ACK to the master.  This message
			should uniquely identify the message it is
			acking (probably by LSN).  This ACK should happen
			even if the log record is not immediately
			processed.

			NOTE: If a replica is receiving log records
			from another replica (for any reason), then
			the master's view of lease management becomes
			more complicated.  I believe  there are two cases.
			Let's assume we have sites Rn, replicas to whom the
			master speaks directly and sites rn who receive log
			records indirectly from other replicas.

			1) If the rn sites can participate in elections,
			then the master *must* consider them in lease
			management.  Therefore, when they receive a log
			record they must still ack the master.

			2) If the rn sites cannot participate in an
			election, then they needn't send ACKs to the
			master.

			So, if a replica is an elector, then it must ACK
			directly to the master.  And, the master must maintain
			state for any replica capable of participating in an
			election.

	Master:		If message is an ACK;
			if ACK is to the message identified in Msg-ID
			set Lease-timeout = Time of Msg-ID + timeout.

In __rep_send_message:

	Replica: Nothing
	Master: if log record: Set Msg-ID and Time_of_Msg-ID for target
		replica(s).

Eager versus Lazy lease maintenance:

In a fairly busy system, we would expect that replicas are constantly
receiving log records from the master, and therefore that the master
lease is regularly being refreshed.  So, if the system is not busy,
how/when does the master refresh leases?

Next, consider the lightly loaded system.  With lazy refresh, the master
does nothing until a read operation is requested.  If it finds that not
enough replicas have valid leases, then the master could send out as
many "ping" requests as necessary to get replicas to  ACK and reset
their lease. (I would re-introduce the ALIVE_REQ and ALIVE messages for
this purpose.)

With eager refresh, the master would notice that leases were about to
expire and would proactively send the pings to keep the leases alive.

Eager refresh requires additional threads on which masters can set
timers to wait. Since we do not necessarily have that without the
replication framework, I'd argue to use lazy refresh capabilities for
now.

6. Replica elections.

A replica may not call and/or participate in an election until its lease
times out.  On one hand, implementing this is simple:

	In __rep_elect, simply sleep until the timeout interval has
	expired and then begin normal election processing.

The question then becomes what to do with messages from the master that
arrive while a replica is waiting to participate in an election.

I would argue that reception of a record from the master should obviate
the need for an election, but this discussion actually seems to be
precisely the conversation that we're having in SR #14752.  The design
here should wait for resolution of that SR (and that SR should probably
be informed by this design, so that we don't preclude master leases in
whatever we decide).

7. Clock Skew

We cannot assume global clocks, so we have to account for clock skew and
nodes running at slightly different clock frequencies in calculating
lease periods.  I would suggest that we take Google's approach, which
is outlined here.

The master interprets the replica's lease timeout from the time the
master last sent a log record or ping that the replica acknowledged.

The replicas, in turn, use their own local time to compute timeouts:
whenever they receive a message from the master, they update their lease
timeout value as reception time + timeout interval.

Both sides must compensate for possible clock skew:

Assume a maximum clock-rate skew among replicas for computing lease
timeouts.

The clock_scale_factor parameter, specified in dbenv->rep_set_master_lease,
is interpreted as follows:

	First, it is a percentage, greater than 100 (this is how we
	sneakily transmit a floating point number as an integer).  So,
	the first thing a site does is divide the clock_scale_factor
	by 100.

	If the slowest replica's clock is a factor of clock_scale_factor
	slower than the fastest clock, then if the faster clock goes
	from time t1 to t2 in X seconds, the slower clock does it in
	clock_scale_factor * X seconds.
	
	A master should set a replica's lease expiration to the start
	time of the sent message + (lease_duration / clock_scale_factor)
	[in case the replica has a slow clock].
	
	A replica should extend its lease to:

		received message time + (lease_duration * clock_scale_factor) 

	[in case this replica has a fast clock].

	The clock_scale_factor should be at least 100.
